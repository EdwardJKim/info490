## Problem 8.2. Twitter Time Series.

- The IPython notebook template for this problem is
  [twitter_series](twitter_series.ipynb).
- When you are done, rename your file to
  `FirstName_LastName_twitter_series.ipynb` and submit it via Moodle.
- In this problem, you will use a real-time stream of tweets
  to track the number of tweets that contain your query phrase.

### Create a Twitter API Connection

First, You have to get Twitter OAuth credentials and obtain API access
  at https://dev.twitter.com/apps/new as detailed in Chapter 1 of
  Mining the Social Web 2nd Edition by Matthew A. Russell
  (hereafter referred to as simply the book),
  and fill in your OAuth credentials in place of the empty
  strings.
  See https://dev.twitter.com/docs/auth/oauth for more information
  on Twitter's OAuth implementation.

```python
CONSUMER_KEY = ''
CONSUMER_SECRET = ''
OAUTH_TOKEN = ''
OAUTH_TOKEN_SECRET = ''

auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,
                           CONSUMER_KEY, CONSUMER_SECRET)
```

The search API (`twitter.Twitter` connector) described in Chapter 1
  of the book is easy to use, but the data you get from the search API
  can be delayed by minutes or even hours.
  But when you want to do a time-series analysis using a *real-time* stream
  of tweets, you can use the streaming API (`twitter.TwitterStream` connector)
  instead.  Note that the streaming data is a 1% random subsample of all tweets
  available. Similar to the search API `twitter.Twitter`,
  the streaming API `twitter.TwitterStream` takes the same
  `twitter.oauth.OAuth` object.

```python
twitter_api = twitter.TwitterStream(auth=auth)
```

### Function: get\_time\_series\_data()

This function is provided for you in the template.

Try a search query and replace the file path if necessary.
  Run it, and grab a snack or study the `get_time_series_data()` function, as
  it may take up to 10 minutes.
  I repeat, do not stop this cell. It takes up to 10 minutes.

After about 10 minutes, the result will be saved to the file named
  `<query>.json`.

```python
with open(os.path.join(fpath, '{0}.json'.format(q)), 'r') as f:
    tweets = json.load(f)
```

If you read the book, you know that `tweets` returned from
  `get_time_series_data()` function
  is a list of dictionaries that contains all the metadata from every tweets we
  fetched.
  You can check this by doing `print(tweets)`, `print(type(tweets))`, and/or
  `print(type(tweets[0]))`.
      
### Function: get_created_at()

Now your task is to

- write a function that takes a list of dictionaries
  (`tweets` or Twitter statuses) and returns a `pandas.Series` object.
    
Each status is a dictionary and has the key `created_at`.
For example, `print(tweets[0]['created_at']` for my data set says
    
    Fri Mar 06 04:31:26 +0000 2015
            
You should use this `created_at` time to create each `DatetimeIndex`
  for `pd.Series`.

In the end, you should return a `pd.Series` with the
  number of tweets created at **each second**.

```python
>>> print(get_created_at(tweets))
2015-03-06 04:31:26    4
2015-03-06 04:31:27    5
2015-03-06 04:31:28    4
2015-03-06 04:31:29    4
2015-03-06 04:31:30    4
2015-03-06 04:31:31    7
2015-03-06 04:31:32    6
2015-03-06 04:31:33    8
2015-03-06 04:31:34    2
2015-03-06 04:31:35    7
2015-03-06 04:31:36    8
2015-03-06 04:31:37    5
2015-03-06 04:31:38    5
2015-03-06 04:31:39    6
2015-03-06 04:31:40    6
...
2015-03-06 04:41:20    2
2015-03-06 04:41:21    3
2015-03-06 04:41:22    1
2015-03-06 04:41:23    1
2015-03-06 04:41:24    2
2015-03-06 04:41:25    1
2015-03-06 04:41:26    5
2015-03-06 04:41:28    1
2015-03-06 04:41:29    3
2015-03-06 04:41:31    3
2015-03-06 04:41:32    4
2015-03-06 04:41:33    1
2015-03-06 04:41:34    2
2015-03-06 04:41:35    2
2015-03-06 04:41:36    2
Length: 570
```

Note that you will get different times and numbers.
  Furthermore, there should be no `NaN` values.

### Plot

### Plot

Finally, we plot the total number of tweets vs time.
  But do you remember that the streaming tweets are only a 1% subsample of the
  entire tweets?
  How would one estimate the total number of tweets from a subsample?
  The simplest way is to multiply the number by 100,
  but this ignores the uncertainties in the number.
  Here, we will use a technique called 
  <a href="http://en.wikipedia.org/wiki/Bootstrapping_(statistics)">Bootstrapping</a>.
  The basic idea is in a time interval of, say, 15 seconds,
  one might have the following bucket of measurements:
             
```python
[4, 5, 4, 4, 4, 7, 6, 8, 2, 7, 8, 5, 5, 6, 6]
```
We randomly pick a number from this bucket, remember the
number, and put it back in the bucket.
We repeat this for $n$ times. After, say, 20 times, we might
end up with

```python
[6.0, 8.0, 8.0, 5.0, 7.0, 4.0, 5.0, 4.0, 4.0, 8.0, 8.0, 8.0,
7.0, 6.0, 5.0, 5.0, 6.0, 4.0, 4.0, 7.0]
```
                
We multiply each element by 100 and build such a resampled
  distribution for every 15 second interval.

```python
np.random.seed(490) # do note change this number
sample = [ts.resample('15S', how=np.random.choice) * 100. for _ in range(20)]
```

- Use this sample in `sns.tsplot()`, which will draw confidence intervals for
  you.
  Note that without bootstrapping (i.e. simply multiplying the numbers by 100)
  you have a single number for each time interval and
  don't get confidence intervals.

Here's an example:

![Time series of Justin Bieber tweets](twitter_ts.png)

Note: Your plot will look different.
